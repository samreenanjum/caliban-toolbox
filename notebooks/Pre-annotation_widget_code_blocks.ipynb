{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep for Crowd Annotation Pipeline\n",
    "\n",
    "1. Collect raw data \n",
    "2. Adjust contrast of images\n",
    "3. Chop up images into manageable pieces\n",
    "4. Make into montages\n",
    "5. Upload to Figure8\n",
    "\n",
    "Files are named by these scripts such that the code blocks can run back-to-back with minimal input. For this reason, it is recommended that users run through the whole pipeline before processing another set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from __future__ import absolute_import\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import Image\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import matplotlib as mpl\n",
    "from skimage import data, filters, io, img_as_uint\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import scipy\n",
    "import sys\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from dcde.pre_annotation.montage_makers import montage_maker, multiple_montage_maker\n",
    "from dcde.pre_annotation.overlapping_chopper import overlapping_crop_dir\n",
    "from dcde.pre_annotation.aws_upload import aws_upload, upload\n",
    "from dcde.pre_annotation.montage_to_csv import csv_maker\n",
    "from dcde.pre_annotation.fig_eight_upload import fig_eight\n",
    "from dcde.pre_annotation.contrast_adjustment import contrast\n",
    "from dcde.utils.io_utils import get_img_names\n",
    "#from dcde.utils.widget_utils import arr2img, choose_img, edit_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes raw images are in .tif stacks, not individual .tif files\n",
    "#optional code block for turning into individual slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adjust contrast of images\n",
    "Before doing anything else, we need to adjust the contrast of the raw data. contrast_adjustment blurs the data using a gaussian filter, finds the edges, inverts, and does additional equalization if needed. The user defines the parameters needed using the widgets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to desired raw directory\n",
    "base_dir = \"/gnv_home/data/Valentine/\"\n",
    "raw_folder = \"Valentine_Svensson_20x_images\"\n",
    "identifier = \"Val_test\"\n",
    "\n",
    "dirpath = os.path.join(base_dir, raw_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that the notebook will play with interactively to run widgets\n",
    "def arr2img(arr):\n",
    "    \"\"\"Display a 2- or 3-d numpy array as an image.\"\"\"\n",
    "    if arr.ndim == 2:\n",
    "        format, cmap = 'png', mpl.cm.gray\n",
    "    elif arr.ndim == 3:\n",
    "        format, cmap = 'jpg', None\n",
    "    else:\n",
    "        raise ValueError(\"Only 2- or 3-d arrays can be displayed as images.\")\n",
    "    # Don't let matplotlib autoscale the color range so we can control overall luminosity\n",
    "    vmax = 255 if arr.dtype == 'uint8' else 1.0\n",
    "    with BytesIO() as buffer:\n",
    "        mpl.image.imsave(buffer, arr, format=format, cmap=cmap)\n",
    "        out = buffer.getvalue()\n",
    "    return Image(out)\n",
    "\n",
    "def choose_img(name):\n",
    "    \"\"\"Used to choose which image we want to use for the widget tester\"\"\"\n",
    "    global img\n",
    "    filepath = os.path.join(dirpath, name)\n",
    "    img = imread(filepath)\n",
    "    return arr2img(img)\n",
    "\n",
    "def edit_image(image, blur=1.0, sobel_toggle = True, sobel_factor = 100, invert_img = True, gamma_adjust = 1.0, equalize_hist=False, equalize_adapthist=False):\n",
    "    \"\"\"Used to edit the image using the widget tester\"\"\"\n",
    "    global sigma\n",
    "    global sobel_option\n",
    "    global sobel\n",
    "    global hist\n",
    "    global adapthist\n",
    "    global gamma\n",
    "    global invert\n",
    "    \n",
    "    new_image = filters.gaussian(image, sigma=blur, multichannel=False)\n",
    "    \n",
    "    if sobel_toggle:\n",
    "        new_image += sobel_factor *sk.filters.sobel(new_image)\n",
    "    new_image = sk.exposure.adjust_gamma(new_image, gamma_adjust, gain = 1)\n",
    "    if invert_img:\n",
    "        new_image[:] = -1.0*new_image[:]\n",
    "    new_image=sk.exposure.rescale_intensity(new_image, in_range = 'image', out_range = 'float')\n",
    "    \n",
    "    if(equalize_hist == True):\n",
    "        #new_image=sk.exposure.rescale_intensity(new_image, in_range = 'image', out_range = 'np.uint16')\n",
    "        new_image = sk.exposure.equalize_hist(new_image, nbins=256, mask=None)\n",
    "        \n",
    "    if(equalize_adapthist == True):\n",
    "        new_image = sk.exposure.equalize_adapthist(new_image, kernel_size=None, clip_limit=0.01, nbins=256)\n",
    "     \n",
    "    new_image = sk.exposure.rescale_intensity(new_image, in_range = 'image', out_range = np.uint8)\n",
    "    new_image = new_image.astype(np.uint8)\n",
    "    \n",
    "    hist = equalize_hist\n",
    "    adapthist = equalize_adapthist\n",
    "    sigma = blur\n",
    "    gamma = gamma_adjust\n",
    "    invert = invert_img\n",
    "    sobel = sobel_factor\n",
    "    sobel_option = sobel_toggle\n",
    "    \n",
    "    return arr2img(new_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97700261b0743c0b6f8a02eb93e2374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('IMG_P1_A1_20x_1.tif', 'IMG_P1_A1_20x_2.tif', 'IMG…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose which raw image you would like to use to test on the contrast adjustment\n",
    "interact(choose_img, name=get_img_names(dirpath));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26884a7b95dd43ab93377a4b25247fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='blur', max=3.0, min=-1.0), Checkbox(value=True, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with choosen image to fix adjustment parameters\n",
    "interact(edit_image, image=fixed(img), sigma=(0.0,4,0.3), gamma_adjust=(0.1,4,0.1), sobel_factor=(10,10000,100));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data will be located at /gnv_home/data/Valentine/Valentine_Svensson_20x_images_contrast_adjusted\n",
      "Processing image 1 of 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float32 to uint16\n",
      "  .format(dtypeobj_in, dtypeobj_out))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 2 of 56\n",
      "Processing image 3 of 56\n",
      "Processing image 4 of 56\n",
      "Processing image 5 of 56\n",
      "Processing image 6 of 56\n",
      "Processing image 7 of 56\n",
      "Processing image 8 of 56\n",
      "Processing image 9 of 56\n",
      "Processing image 10 of 56\n",
      "Processing image 11 of 56\n",
      "Processing image 12 of 56\n",
      "Processing image 13 of 56\n",
      "Processing image 14 of 56\n",
      "Processing image 15 of 56\n",
      "Processing image 16 of 56\n",
      "Processing image 17 of 56\n",
      "Processing image 18 of 56\n",
      "Processing image 19 of 56\n",
      "Processing image 20 of 56\n",
      "Processing image 21 of 56\n",
      "Processing image 22 of 56\n",
      "Processing image 23 of 56\n",
      "Processing image 24 of 56\n",
      "Processing image 25 of 56\n",
      "Processing image 26 of 56\n",
      "Processing image 27 of 56\n",
      "Processing image 28 of 56\n",
      "Processing image 29 of 56\n",
      "Processing image 30 of 56\n",
      "Processing image 31 of 56\n",
      "Processing image 32 of 56\n",
      "Processing image 33 of 56\n",
      "Processing image 34 of 56\n",
      "Processing image 35 of 56\n",
      "Processing image 36 of 56\n",
      "Processing image 37 of 56\n",
      "Processing image 38 of 56\n",
      "Processing image 39 of 56\n",
      "Processing image 40 of 56\n",
      "Processing image 41 of 56\n",
      "Processing image 42 of 56\n",
      "Processing image 43 of 56\n",
      "Processing image 44 of 56\n",
      "Processing image 45 of 56\n",
      "Processing image 46 of 56\n",
      "Processing image 47 of 56\n",
      "Processing image 48 of 56\n",
      "Processing image 49 of 56\n",
      "Processing image 50 of 56\n",
      "Processing image 51 of 56\n",
      "Processing image 52 of 56\n",
      "Processing image 53 of 56\n",
      "Processing image 54 of 56\n",
      "Processing image 55 of 56\n",
      "Processing image 56 of 56\n"
     ]
    }
   ],
   "source": [
    "# With choosen parameters, process all the raw data in the folder\n",
    "contrast(base_dir, raw_folder, identifier, sigma, hist, adapthist, gamma, sobel_option, sobel, invert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chop up images into manageable pieces\n",
    "\n",
    "Each full-size image usually has many cells in it. This makes them difficult to fully annotate! For ease of annotation (and better results), each frame is chopped up into smaller, overlapping frames, ultimately creating a set of movies. \n",
    "\n",
    "These smaller movies can be made with overlapping edges, making it easier to stitch annotations together into one large annotated movie (in the post-annotation pipeline). A large overlap will result in redundant annotations.\n",
    "\n",
    "Even if you want to process the full-sized image, run the chopper with num_segments of 1. The montage makers are written to run on the output of the chopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_direc = \"/home/geneva/Desktop/Nb_testing/\"\n",
    "# raw_direc = os.path.join(base_direc, \"MouseBrain_s7_nuclear\")\n",
    "# identifier = \"MouseBrain_s7_nuc\"\n",
    "\n",
    "num_x_segments = 5\n",
    "num_y_segments = 5\n",
    "overlap_perc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Image Size:  (567, 1171)\n",
      "Correct? (y/n): y\n",
      "Your new images will be  280  pixels by  135  pixels big.\n",
      "Processing...\n",
      "Cropped files saved to /gnv_home/contrast_test/pics181220/raw_contrast_adjusted_chopped_5_5\n"
     ]
    }
   ],
   "source": [
    "raw_direc = \"/gnv_home/contrast_test/pics181220/raw_contrast_adjusted\"\n",
    "overlapping_crop_dir(raw_direc, identifier, num_x_segments, num_y_segments, overlap_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make into montages\n",
    "multiple_montage_maker is written to run on the output of the chopper, ie the folder where each chopped movie folder is saved. It will make montages of each subfolder according to the variables specified. It will make more than one montage per subfolder if there are enough frames to do so.\n",
    "\n",
    "The variables used in multiple_montage_maker are saved in a JSON file so they can be reused in post-annotation processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "montage_len = 10\n",
    "\n",
    "direc = raw_direc + \"_chopped_\" + str(num_x_segments) + \"_\" + str(num_y_segments)\n",
    "#direc = \"/home/geneva/Desktop/Nb_testing/nuclear_test_chopped_4_4\"\n",
    "\n",
    "save_direc = os.path.join(base_dir, identifier + \"_montages_\" + str(num_x_segments) + \"_\" + str(num_y_segments))\n",
    "#save_direc = \"/home/geneva/Desktop/Nb_testing/montages\"\n",
    "\n",
    "log_direc = os.path.join(base_dir, \"json_logs\")\n",
    "\n",
    "row_length = 5\n",
    "x_buffer = 5\n",
    "y_buffer = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now montaging images from: test_x_00_y_00\n",
      "You will be able to make 1 montages from this movie.\n",
      "The last 1 frames will not be used in a montage. \n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (122,313) into shape (135,280)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f9839ef4384f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m multiple_montage_maker(montage_len, direc, save_direc, identifier, \n\u001b[0;32m----> 2\u001b[0;31m                        num_x_segments, num_y_segments, row_length, x_buffer, y_buffer, log_direc)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/dcde/pre_annotation/montage_makers.py\u001b[0m in \u001b[0;36mmultiple_montage_maker\u001b[0;34m(montage_len, direc, save_direc, identifier, num_x_segments, num_y_segments, row_length, x_buffer, y_buffer, log_direc)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Now montaging images from: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0midentifier\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_x_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_y_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mnum_montages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmontage_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmontage_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_direc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_direc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m#logged_name = identifier + \"_x_\" + str(x_pos) + \"_y_\" + str(y_pos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/dcde/pre_annotation/montage_makers.py\u001b[0m in \u001b[0;36mmontage_maker\u001b[0;34m(montage_len, stack_direc, save_direc, identifier, x_pos, y_pos, row_length, x_buffer, y_buffer)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;31m#add image to montage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mmontage_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mx_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m#save montage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (122,313) into shape (135,280)"
     ]
    }
   ],
   "source": [
    "multiple_montage_maker(montage_len, direc, save_direc, identifier, \n",
    "                       num_x_segments, num_y_segments, row_length, x_buffer, y_buffer, log_direc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload to Figure Eight\n",
    "Now that the images are processed into montages, they need to be uploaded to an AWS bucket and submitted to Figure Eight. This involves uploading the files to AWS, making a CSV file with the links to the uploaded images, and using that CSV file to create a Figure Eight job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload files to AWS\n",
    "aws_upload will look for image files in the specified directory (folder_to_upload, set by default to be wherever the output of multiple_montage_maker was saved) and upload them into a bucket.\n",
    "\n",
    "For the Van Valen lab, the default bucket is \"figure-eight-deepcell\" and keys (aws_folder + file names) correspond to the file structure of our data server.\n",
    "\n",
    "aws_upload returns a list of the urls to which images were uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "bucket_name = \"figure-eight-deepcell\" #default\n",
    "aws_folder = \"MouseBrain/set7\"\n",
    "folder_to_upload = save_direc #usually .../montages\n",
    "#data_to_upload = \"/home/geneva/Desktop/Nb_testing/montages/\"\n",
    "\n",
    "uploaded_montages = aws_upload(bucket_name, aws_folder, folder_to_upload)\n",
    "\n",
    "#os.path.join(\"https://s3.us-east-2.amazonaws.com\", bucket_name, aws_folder)\n",
    "#print(uploaded_montages)\n",
    "#from io_utils import get_img_names\n",
    "#imgs_to_upload = get_img_names(folder_to_upload)\n",
    "#for index, img in enumerate(imgs_to_upload):\n",
    "#    print(img)\n",
    "#    print(os.path.join(folder_to_upload, img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CSV file\n",
    "Figure Eight jobs can be created easily by using a CSV file where each row contains information about one task. For our jobs, each row has the link to the location of one montage, and information about that montage (currently, just the \"identifier\" specified at the beginning of the pipeline). The CSV file is saved as \"identifier\".csv in a folder that only holds CSVs. CSV folders are usually in cell-type directories, so identifiers should be able to distinguish between sets, parts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier = \"test\"\n",
    "csv_direc = os.path.join(base_direc, \"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_maker(uploaded_montages, identifier, csv_direc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Figure Eight job\n",
    "The Figure Eight API allows us to create a new job and upload data to it from this notebook. However, since our jobs don't include required test questions, editing job information such as the title of the job must be done via the website. This section of the notebook uses the API to create a job and upload data to it, then reminds the user to finish editing the job on the website.\n",
    "\n",
    "Some sample job IDs to copy are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_id_to_copy = 1344258 #Elowitz timelapse RFP pilot\n",
    "job_id_to_copy = 1346216 #Deepcell MouseBrain 3x5\n",
    "#job_id_to_copy = 1306431 #Deepcell overlapping Mibi\n",
    "#job_id_to_copy = 1292179 #Deepcell HEK\n",
    "#job_id_to_copy ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcde.pre_annotation.fig_eight_upload import fig_eight\n",
    "\n",
    "fig_eight(csv_direc, identifier, job_id_to_copy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
