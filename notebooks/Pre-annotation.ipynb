{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep for Crowd Annotation Pipeline\n",
    "\n",
    "1. Collect raw data \n",
    "2. Adjust contrast of images\n",
    "3. Chop up images into manageable pieces\n",
    "4. Make into montages\n",
    "5. Upload to Figure8\n",
    "\n",
    "Files are named by these scripts such that the code blocks can run back-to-back with minimal input. For this reason, it is recommended that users run through the whole pipeline before processing another set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from io import BytesIO\n",
    "\n",
    "from IPython.display import Image\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import matplotlib as mpl\n",
    "from skimage import data, filters, io, img_as_uint\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import os\n",
    "from scipy import ndimage\n",
    "import scipy\n",
    "import sys\n",
    "from ipywidgets import interact\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from dcde.pre_annotation.montage_makers import montage_maker, multiple_montage_maker\n",
    "from dcde.pre_annotation.overlapping_chopper import overlapping_crop_dir\n",
    "from dcde.pre_annotation.aws_upload import aws_upload, upload\n",
    "from dcde.pre_annotation.montage_to_csv import csv_maker\n",
    "from dcde.pre_annotation.fig_eight_upload import fig_eight\n",
    "from dcde.pre_annotation.contrast_adjustment import contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sometimes raw images are in .tif stacks, not individual .tif files\n",
    "#optional code block for turning into individual slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adjust contrast of images\n",
    "Before doing anything else, we need to adjust the contrast of the raw data. contrast_adjustment blurs the data using a gaussian filter, finds the edges, inverts, and does additional equalization if needed. The user defines the parameters needed using the widgets below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to desired raw directory\n",
    "base_dir = \"test/pics\"\n",
    "raw_folder = \"pics181220\"\n",
    "identifier = \"test\"\n",
    "\n",
    "dirpath = os.path.join(base_dir, raw_folder + \"/raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that the notebook will play with interactively to run widgets\n",
    "def arr2img(arr):\n",
    "    \"\"\"Display a 2- or 3-d numpy array as an image.\"\"\"\n",
    "    if arr.ndim == 2:\n",
    "        format, cmap = 'png', mpl.cm.gray\n",
    "    elif arr.ndim == 3:\n",
    "        format, cmap = 'jpg', None\n",
    "    else:\n",
    "        raise ValueError(\"Only 2- or 3-d arrays can be displayed as images.\")\n",
    "    # Don't let matplotlib autoscale the color range so we can control overall luminosity\n",
    "    vmax = 255 if arr.dtype == 'uint8' else 1.0\n",
    "    with BytesIO() as buffer:\n",
    "        mpl.image.imsave(buffer, arr, format=format, cmap=cmap)\n",
    "        out = buffer.getvalue()\n",
    "    return Image(out)\n",
    "\n",
    "def choose_img(name):\n",
    "    \"\"\"Used to choose which image we want to use for the widget tester\"\"\"\n",
    "    global img\n",
    "    filepath = os.path.join(dirpath, name)\n",
    "    img = img_as_uint(imread(filepath))\n",
    "    return arr2img(img)\n",
    "\n",
    "def edit_image(image, sigma=1.0, equalize_hist=False, equalize_adapthist=False):\n",
    "    \"\"\"Used to edit the image using the widget tester\"\"\"\n",
    "    global hist\n",
    "    global adapthist\n",
    "    global gaussian_sigma\n",
    "    \n",
    "    new_image = filters.gaussian(image, sigma=sigma, multichannel=False)\n",
    "    new_image += 1000*sk.filters.sobel(new_image)\n",
    "    new_image[:] = -1.0*new_image[:]\n",
    "    new_image=sk.exposure.rescale_intensity(new_image, in_range = 'image', out_range = 'float')\n",
    "    \n",
    "    if(equalize_hist == True):\n",
    "        #new_image=sk.exposure.rescale_intensity(new_image, in_range = 'image', out_range = 'np.uint16')\n",
    "        new_image = sk.exposure.equalize_hist(new_image, nbins=256, mask=None)\n",
    "        \n",
    "    if(equalize_adapthist == True):\n",
    "        new_image = sk.exposure.equalize_adapthist(new_image, kernel_size=None, clip_limit=0.01, nbins=256)\n",
    "     \n",
    "    \n",
    "    hist = equalize_hist\n",
    "    adapthist = equalize_adapthist\n",
    "    gaussian_sigma = sigma\n",
    "        \n",
    "    \n",
    "    return arr2img(new_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52aabf557da5484e80a8c692fe20d20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='name', options=('181220_Hyb1_pos1_4umSteps-0001.tif', '181220_Hyb1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose which raw image you would like to use to test on the contrast adjustment\n",
    "interact(choose_img, name=sorted(set(os.listdir(dirpath))));\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8866cbd56d534a0caa7dfd094cf7dfa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='sigma', max=4.0, step=0.3), Checkbox(value=False, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test with choosen image to fix adjustment parameters\n",
    "interact(edit_image, image=fixed(img), sigma=(0.0,4,0.3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data will be located at test/pics/pics181220_contrast_adjusted\n"
     ]
    }
   ],
   "source": [
    "# With choosen parameters, process all the raw data in the folder\n",
    "contrast(base_dir, raw_folder, identifier, gaussian_sigma, hist, adapthist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chop up images into manageable pieces\n",
    "\n",
    "Each full-size image usually has many cells in it. This makes them difficult to fully annotate! For ease of annotation (and better results), each frame is chopped up into smaller, overlapping frames, ultimately creating a set of movies. \n",
    "\n",
    "These smaller movies can be made with overlapping edges, making it easier to stitch annotations together into one large annotated movie (in the post-annotation pipeline). A large overlap will result in redundant annotations.\n",
    "\n",
    "Even if you want to process the full-sized image, run the chopper with num_segments of 1. The montage makers are written to run on the output of the chopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_direc = \"/home/geneva/Desktop/Nb_testing/\"\n",
    "# raw_direc = os.path.join(base_direc, \"MouseBrain_s7_nuclear\")\n",
    "# identifier = \"MouseBrain_s7_nuc\"\n",
    "\n",
    "num_x_segments = 5\n",
    "num_y_segments = 5\n",
    "overlap_perc = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_direc = \"test/pics/pics181220/processed\"\n",
    "overlapping_crop_dir(raw_direc, identifier, num_x_segments, num_y_segments, overlap_perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make into montages\n",
    "multiple_montage_maker is written to run on the output of the chopper, ie the folder where each chopped movie folder is saved. It will make montages of each subfolder according to the variables specified. It will make more than one montage per subfolder if there are enough frames to do so.\n",
    "\n",
    "The variables used in multiple_montage_maker are saved in a JSON file so they can be reused in post-annotation processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "montage_len = 15\n",
    "\n",
    "direc = raw_direc + \"_chopped_\" + str(num_x_segments) + \"_\" + str(num_y_segments)\n",
    "#direc = \"/home/geneva/Desktop/Nb_testing/nuclear_test_chopped_4_4\"\n",
    "\n",
    "save_direc = os.path.join(base_direc, identifier + \"_montages_\" + str(num_x_segments) + \"_\" + str(num_y_segments))\n",
    "#save_direc = \"/home/geneva/Desktop/Nb_testing/montages\"\n",
    "\n",
    "log_direc = os.path.join(base_direc, \"json_logs\")\n",
    "\n",
    "row_length = 5\n",
    "x_buffer = 5\n",
    "y_buffer = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "multiple_montage_maker(montage_len, direc, save_direc, identifier, \n",
    "                       num_x_segments, num_y_segments, row_length, x_buffer, y_buffer, log_direc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload to Figure Eight\n",
    "Now that the images are processed into montages, they need to be uploaded to an AWS bucket and submitted to Figure Eight. This involves uploading the files to AWS, making a CSV file with the links to the uploaded images, and using that CSV file to create a Figure Eight job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload files to AWS\n",
    "aws_upload will look for image files in the specified directory (folder_to_upload, set by default to be wherever the output of multiple_montage_maker was saved) and upload them into a bucket.\n",
    "\n",
    "For the Van Valen lab, the default bucket is \"figure-eight-deepcell\" and keys (aws_folder + file names) correspond to the file structure of our data server.\n",
    "\n",
    "aws_upload returns a list of the urls to which images were uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "bucket_name = \"figure-eight-deepcell\" #default\n",
    "aws_folder = \"MouseBrain/set7\"\n",
    "folder_to_upload = save_direc #usually .../montages\n",
    "#data_to_upload = \"/home/geneva/Desktop/Nb_testing/montages/\"\n",
    "\n",
    "uploaded_montages = aws_upload(bucket_name, aws_folder, folder_to_upload)\n",
    "\n",
    "#os.path.join(\"https://s3.us-east-2.amazonaws.com\", bucket_name, aws_folder)\n",
    "#print(uploaded_montages)\n",
    "#from io_utils import get_img_names\n",
    "#imgs_to_upload = get_img_names(folder_to_upload)\n",
    "#for index, img in enumerate(imgs_to_upload):\n",
    "#    print(img)\n",
    "#    print(os.path.join(folder_to_upload, img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CSV file\n",
    "Figure Eight jobs can be created easily by using a CSV file where each row contains information about one task. For our jobs, each row has the link to the location of one montage, and information about that montage (currently, just the \"identifier\" specified at the beginning of the pipeline). The CSV file is saved as \"identifier\".csv in a folder that only holds CSVs. CSV folders are usually in cell-type directories, so identifiers should be able to distinguish between sets, parts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifier = \"test\"\n",
    "csv_direc = os.path.join(base_direc, \"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_maker(uploaded_montages, identifier, csv_direc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Figure Eight job\n",
    "The Figure Eight API allows us to create a new job and upload data to it from this notebook. However, since our jobs don't include required test questions, editing job information such as the title of the job must be done via the website. This section of the notebook uses the API to create a job and upload data to it, then reminds the user to finish editing the job on the website.\n",
    "\n",
    "Some sample job IDs to copy are provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job_id_to_copy = 1344258 #Elowitz timelapse RFP pilot\n",
    "job_id_to_copy = 1346216 #Deepcell MouseBrain 3x5\n",
    "#job_id_to_copy = 1306431 #Deepcell overlapping Mibi\n",
    "#job_id_to_copy = 1292179 #Deepcell HEK\n",
    "#job_id_to_copy ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dcde.pre_annotation.fig_eight_upload import fig_eight\n",
    "\n",
    "fig_eight(csv_direc, identifier, job_id_to_copy)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
