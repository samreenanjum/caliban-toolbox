{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caliban Fig8 Upload Pipeline\n",
    "This pipeline creates a Figure Eight job and uploads data files to an S3 bucket for data curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "\n",
    "from ipywidgets import fixed, interactive\n",
    "from imageio import imread\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from deepcell_toolbox.pre_annotation.aws_upload import aws_caliban_upload, caliban_upload\n",
    "from deepcell_toolbox.pre_annotation.caliban_csv import csv_maker\n",
    "from deepcell_toolbox.pre_annotation.fig_eight_upload import fig_eight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create npz (if needed)\n",
    "\n",
    "We'll skip this for now since we already have a lot of npzs created. Also, the fig8 image download pipeline includes create_training_data. This part would mostly be for creating npz files from model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trk (if needed)\n",
    "Similar to creating npz. This part could probably take in an npz or the raw images + annotations. Adding an existing lineage should be optional. Currently, deepcell.org returns raw, annotations, and lineage zipped together (so that would be the lineage that goes into the trk), but this part could also just make a lineage analogous to Caliban's save_as_trk option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split npz into pieces (semi-optional)\n",
    "The idea is that you'd be starting from one huge npz and breaking it into managable pieces. This part will probably include:\n",
    "- reshape npz (size of each piece is smaller, but same number of frames)\n",
    "- break up each npz into fewer frames (annotator does not necessarily need to do all 30+ frames of a movie at once)\n",
    "- save these reshaped pieces as individual npz files, so they can be uploaded and worked on separately\n",
    "- relabel the npzs as needed (choose between no relabel, relabel each cell in each frame to have unique label, or relabel to have unique labels but preserve relationships across \n",
    "\n",
    "These pieces will need specific names so that we can put them back together again if needed (especially putting frames back into longer contiguous movies).\n",
    "\n",
    "(parts of this are already written - Geneva)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload pieces to AWS\n",
    "\n",
    "### Select directory\n",
    "\n",
    "\"base_dir\" is a directory that holds the desired .npz or .trk file folder for data curation. This will go through the folder and upload the files to the desired S3 input bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/jannie/Desktop/data\"\n",
    "\n",
    "# folder of files to curate with Figure Eight\n",
    "data_folder = \"first_pass_npz\"\n",
    "\n",
    "data_dir = os.path.join(base_dir, data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to AWS\n",
      "f8test\n",
      "/Users/jannie/Desktop/data/first_pass_npz\n",
      "['HeLa-S3_cyto_movie_s1_batch_00_first_pass.npz', 'HeLa-S3_cyto_movie_s1_batch_01_first_pass.npz']\n",
      "HeLa-S3_cyto_movie_s1_batch_00_first_pass.npz\n",
      "/Users/jannie/Desktop/data/first_pass_npz/HeLa-S3_cyto_movie_s1_batch_00_first_pass.npz  31104450 / 31104450.0  (100.00%)\n",
      "\n",
      "HeLa-S3_cyto_movie_s1_batch_01_first_pass.npz\n",
      "/Users/jannie/Desktop/data/first_pass_npz/HeLa-S3_cyto_movie_s1_batch_01_first_pass.npz  31104450 / 31104450.0  (100.00%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bucket to load uncurated files\n",
    "input_bucket = \"caliban-input\"\n",
    "\n",
    "# bucket for curated files after submission\n",
    "output_bucket = \"caliban-output\"\n",
    "\n",
    "# subfolders in input/output bucket\n",
    "aws_folder = \"f8test\"\n",
    "\n",
    "uploaded_images = aws_caliban_upload(input_bucket, output_bucket, aws_folder, folder_to_upload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV file for Figure Eight\n",
    "\n",
    "\n",
    "Figure Eight jobs can be created easily by using a CSV file where each row contains information about one task. To create jobs for caliban, each row has a unique url that directs users to the Caliban tool with the correct data to curate. The CSV file is saved as {identifier}_upload.csv in a folder that only holds CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "identifier = \"test_set\"\n",
    "csv_maker(uploaded_images, csv_dir, identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Figure Eight job\n",
    "\n",
    "The Figure Eight API allows us to create a new job and upload data to it from this notebook. However, since our jobs don't include required test questions, editing job information such as the title of the job must be done via the website. This section of the notebook uses the API to create a job and upload data to it, then reminds the user to finish editing the job on the website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure eight api key? ········\n",
      "New job ID is: 1451551\n",
      "Added data\n",
      "Now that the data is added, you should go to the Figure Eight website to: \n",
      "-change the job title \n",
      "-review the job design \n",
      "-confirm pricing \n",
      "-launch the job (or contact success manager)\n"
     ]
    }
   ],
   "source": [
    "job_id_to_copy =  1432466 #Caliban test job\n",
    "fig_eight(csv_dir, identifier, job_id_to_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
